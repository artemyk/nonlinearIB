{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import time, os, pickle, pathlib\n",
    "\n",
    "import model\n",
    "import trainutils\n",
    "\n",
    "report_every = 10  # how often to print stats during training\n",
    "n_runs       = 1   # how many times to repeat the whole scan across beta's\n",
    "savedirbase  = str(pathlib.Path().absolute()) + '/saveddata/'\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)\n",
    "\n",
    "base_cfg = {\n",
    "    'n_batch'         : 128 ,       # SGD batch size\n",
    "    'train_noisevar'  : 'gradient', # train noise variance with gradient descent ('gradient'), \n",
    "                                    #  scipy optimizer loop ('scipy'), or leave fixed ('none')\n",
    "    'n_noisevar_batch': 1000,       # batch size for training noise variance when train_noisevar='scipy'\n",
    "    'initial_fitvar'  : False,      # whether to set noisevar to optimal value before training\n",
    "    'squaredIB'       : True,       # optimize I(Y;T)-beta*I(X;T) or I(Y;T)-beta*I(X;T)^2 \n",
    "    'err_func'        : 'softmax_ce', # 'softmax_ce' for classification, 'mse' for regression  \n",
    "}\n",
    "\n",
    "\n",
    "runtype = 'MNIST'\n",
    "#runtype = 'NoisyClassifier'\n",
    "#runtype = 'Regression'\n",
    "assert(runtype in ['MNIST', 'NoisyClassifier', 'Regression'])\n",
    "\n",
    "if runtype == 'MNIST':\n",
    "    data = trainutils.load_mnist()\n",
    "    savedir = runtype + '/v2'\n",
    "    cfg = {\n",
    "        'input_dims'  : 784,\n",
    "        'entropyY'    : np.log(10), \n",
    "        'n_epochs'    : 150,\n",
    "        'squaredIB'   : True,\n",
    "        'encoder_arch': [(512,tf.nn.relu),(512,tf.nn.relu),(2,None)], \n",
    "        'decoder_arch': [(512,tf.nn.relu),(10,None)],\n",
    "    }\n",
    "\n",
    "elif runtype == 'NoisyClassifier':\n",
    "    savedir = runtype + '/v5sq'\n",
    "    # Data from artificial dataset used in Schwartz-Ziv and Tishby\n",
    "    d1 = scipy.io.loadmat('data/g1.mat')\n",
    "    d2 = scipy.io.loadmat('data/g2.mat')\n",
    "    data = { 'trn_X' : d1['F'].astype('float32'), 'trn_Y': one_hot(d1['y'].flat),\n",
    "             'tst_X' : d2['F'].astype('float32'), 'tst_Y': one_hot(d2['y'].flat)}\n",
    "    cfg = {\n",
    "        'input_dims'    : 12,\n",
    "        'entropyY'      : np.log(2),\n",
    "        'n_epochs'      : 200,\n",
    "        'squaredIB'     : True,\n",
    "        'encoder_arch'  : [(20,tf.nn.relu),(20,tf.nn.relu),(2,None)], \n",
    "        'decoder_arch'  : [(20,tf.nn.relu),(2,None)],\n",
    "    }\n",
    "    \n",
    "elif runtype == 'Regression':\n",
    "    savedir = runtype + '/v5sq-eta'\n",
    "    # data generated by makeregressiondata.py\n",
    "    with open('data/regression-100-10.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    labelcov = np.cov(data['trn_Y'].T)\n",
    "    entropyY = 0.5 * np.log(np.linalg.det(2*np.pi*np.exp(1)*labelcov))\n",
    "    cfg = {\n",
    "        'input_dims'       : data['trn_X'].shape[1],\n",
    "        'entropyY'         : entropyY,\n",
    "        'n_epochs'         : 200,\n",
    "        'encoder_arch'     : [(100,tf.nn.relu),(100,tf.nn.relu),(2,None)], \n",
    "        'decoder_arch'     : [(100,tf.nn.relu),(10,None)],\n",
    "        'err_func'         : 'mse',\n",
    "        'squaredIB'        : True,\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    raise Exception('unknown runtype')\n",
    "    \n",
    "savedir = savedirbase + savedir\n",
    "for k, v in base_cfg.items():\n",
    "    if k not in cfg: \n",
    "        cfg[k] = v\n",
    "cfg['optimizer'] = repr(optimizer)\n",
    "\n",
    "if cfg['squaredIB']:\n",
    "    betavals = 10**np.linspace(-3, .5, 10, endpoint=True)\n",
    "else:\n",
    "    betavals = 10**np.linspace(-1, 0., 10, endpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess=tf.Session()\n",
    "\n",
    "\n",
    "n = model.Net(input_dims   = cfg['input_dims'],\n",
    "              encoder_arch = cfg['encoder_arch'], \n",
    "              decoder_arch = cfg['decoder_arch'],\n",
    "              err_func     = cfg['err_func'],\n",
    "              entropyY     = cfg['entropyY'],\n",
    "              trainable_noisevar = cfg['train_noisevar']=='gradient', \n",
    "              noisevar     = 0.01)\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model\n",
      "\n",
      "mode: ce epoch: 1 | beta: 0.0000 | noisevar: 0.01 | kw: 0.0137392\n",
      "ce:  2.308/ 2.307 | acc:  0.097/ 0.101 | loss:  2.308/ 2.307 | \n",
      "Ixt:  3.291/ 3.251 | Ixt_lb:  2.081/ 2.050 | vIxt:  9.138/ 9.116 | Iyt: -0.005/-0.004 | \n",
      "\n",
      "mode: ce epoch: 11 | beta: 0.0000 | noisevar: 0.01 | kw: 1.63276\n",
      "ce:  0.066/ 0.082 | acc:  0.980/ 0.974 | loss:  0.066/ 0.082 | \n",
      "Ixt:  10.180/ 10.191 | Ixt_lb:  9.196/ 9.201 | vIxt:  343.521/ 332.489 | Iyt:  2.236/ 2.220 | \n",
      "\n",
      "mode: ce epoch: 21 | beta: 0.0000 | noisevar: 0.01 | kw: 2.28921\n",
      "ce:  0.032/ 0.026 | acc:  0.990/ 0.991 | loss:  0.032/ 0.026 | \n",
      "Ixt:  10.622/ 10.612 | Ixt_lb:  9.629/ 9.612 | vIxt:  562.379/ 563.456 | Iyt:  2.271/ 2.277 | \n",
      "\n",
      "mode: ce epoch: 31 | beta: 0.0000 | noisevar: 0.01 | kw: 3.33712\n",
      "ce:  0.031/ 0.037 | acc:  0.989/ 0.989 | loss:  0.031/ 0.037 | \n",
      "Ixt:  10.881/ 10.905 | Ixt_lb:  9.925/ 9.952 | vIxt:  852.996/ 872.600 | Iyt:  2.272/ 2.265 | \n",
      "\n",
      "mode: ce epoch: 41 | beta: 0.0000 | noisevar: 0.01 | kw: 5.4792\n",
      "ce:  0.015/ 0.012 | acc:  0.994/ 0.995 | loss:  0.015/ 0.012 | \n",
      "Ixt:  11.345/ 11.355 | Ixt_lb:  10.372/ 10.383 | vIxt:  1249.973/ 1278.312 | Iyt:  2.287/ 2.290 | \n",
      "\n",
      "mode: ce epoch: 51 | beta: 0.0000 | noisevar: 0.01 | kw: 8.30931\n",
      "ce:  0.034/ 0.013 | acc:  0.993/ 0.997 | loss:  0.034/ 0.013 | \n",
      "Ixt:  11.692/ 11.693 | Ixt_lb:  10.755/ 10.765 | vIxt:  1837.505/ 1792.602 | Iyt:  2.269/ 2.289 | \n",
      "\n",
      "mode: ce epoch: 61 | beta: 0.0000 | noisevar: 0.01 | kw: 7.67176\n",
      "ce:  0.010/ 0.008 | acc:  0.997/ 0.998 | loss:  0.010/ 0.008 | \n",
      "Ixt:  11.854/ 11.867 | Ixt_lb:  10.904/ 10.926 | vIxt:  2514.700/ 2624.022 | Iyt:  2.293/ 2.295 | \n",
      "\n",
      "mode: ce epoch: 71 | beta: 0.0000 | noisevar: 0.01 | kw: 10.209\n",
      "ce:  0.003/ 0.009 | acc:  0.999/ 0.997 | loss:  0.003/ 0.009 | \n",
      "Ixt:  12.213/ 12.239 | Ixt_lb:  11.275/ 11.282 | vIxt:  3508.823/ 3514.973 | Iyt:  2.299/ 2.294 | \n",
      "\n",
      "mode: ce epoch: 81 | beta: 0.0000 | noisevar: 0.01 | kw: 19.132\n",
      "ce:  0.007/ 0.005 | acc:  0.998/ 0.998 | loss:  0.007/ 0.005 | \n",
      "Ixt:  12.474/ 12.509 | Ixt_lb:  11.564/ 11.577 | vIxt:  4007.648/ 3803.169 | Iyt:  2.296/ 2.298 | \n",
      "\n",
      "mode: ce epoch: 91 | beta: 0.0000 | noisevar: 0.01 | kw: 26.1198\n",
      "ce:  0.027/ 0.026 | acc:  0.992/ 0.993 | loss:  0.027/ 0.026 | \n",
      "Ixt:  12.710/ 12.742 | Ixt_lb:  11.776/ 11.807 | vIxt:  5483.177/ 5513.172 | Iyt:  2.275/ 2.277 | \n",
      "\n",
      "mode: ce epoch: 101 | beta: 0.0000 | noisevar: 0.01 | kw: 29.4533\n",
      "ce:  0.023/ 0.010 | acc:  0.996/ 0.998 | loss:  0.023/ 0.010 | \n",
      "Ixt:  12.892/ 12.897 | Ixt_lb:  11.976/ 11.985 | vIxt:  8013.208/ 8418.632 | Iyt:  2.280/ 2.293 | \n",
      "\n",
      "mode: ce epoch: 111 | beta: 0.0000 | noisevar: 0.01 | kw: 25.3748\n",
      "ce:  0.009/ 0.019 | acc:  0.998/ 0.996 | loss:  0.009/ 0.019 | \n",
      "Ixt:  13.050/ 13.048 | Ixt_lb:  12.103/ 12.118 | vIxt:  9080.537/ 8912.547 | Iyt:  2.294/ 2.283 | \n",
      "\n",
      "mode: ce epoch: 121 | beta: 0.0000 | noisevar: 0.01 | kw: 47.6967\n",
      "ce:  0.011/ 0.028 | acc:  0.994/ 0.994 | loss:  0.011/ 0.028 | \n",
      "Ixt:  13.487/ 13.452 | Ixt_lb:  12.562/ 12.527 | vIxt:  11421.350/ 11005.646 | Iyt:  2.291/ 2.275 | \n",
      "\n",
      "mode: ce epoch: 131 | beta: 0.0000 | noisevar: 0.01 | kw: 76.008\n",
      "ce:  0.012/ 0.015 | acc:  0.996/ 0.996 | loss:  0.012/ 0.015 | \n",
      "Ixt:  13.584/ 13.589 | Ixt_lb:  12.655/ 12.648 | vIxt:  10353.656/ 9881.993 | Iyt:  2.290/ 2.288 | \n",
      "\n",
      "mode: ce epoch: 141 | beta: 0.0000 | noisevar: 0.01 | kw: 87.6378\n",
      "ce:  0.021/ 0.019 | acc:  0.995/ 0.993 | loss:  0.021/ 0.019 | \n",
      "Ixt:  13.656/ 13.713 | Ixt_lb:  12.716/ 12.769 | vIxt:  14241.265/ 15125.587 | Iyt:  2.282/ 2.283 | \n",
      "\n",
      "mode: ce epoch: 151 | beta: 0.0000 | noisevar: 0.01 | kw: 73.5077\n",
      "ce:  0.028/ 0.028 | acc:  0.996/ 0.992 | loss:  0.028/ 0.028 | \n",
      "Ixt:  13.764/ 13.749 | Ixt_lb:  12.833/ 12.818 | vIxt:  17338.363/ 16812.957 | Iyt:  2.275/ 2.275 | \n",
      "Model saved in path: /home/artemy/nonlinearIB/code/saveddata/MNIST/v2/_tf_basemodel\n"
     ]
    }
   ],
   "source": [
    "print(\"Making base model\")\n",
    "#cfg2['n_epochs'] = 10\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "trainutils.train(sess, 'ce', 0.0, cfg, data, n, optimizer, report_every, fname=savedir+'/results-base')\n",
    "\n",
    "save_path = saver.save(sess, savedir+'/_tf_basemodel')\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess  = tf.Session()\n",
    "#n = model.Net(encoder_arch=[(512,'relu'),(512,'relu'),(2,'relu')], decoder_arch=[(512,'relu'),],\n",
    "#              trainable_sigma=cfg['train_sigma'], log_sigma2=-2, log_eta2=-20, init_beta=0.0)\n",
    "#loader = tf.train.import_meta_graph(basemodelpath+'.meta')\n",
    "#saver.restore(sess, basemodelpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing VIB, beta=0.0010\n",
      "\n",
      "mode: VIB epoch: 1 | beta: 0.0010 | noisevar: 0.01 | kw: 48.2735\n",
      "ce:  0.046/ 0.041 | acc:  0.991/ 0.993 | loss:  325164.250/ 342443.438 | \n",
      "Ixt:  13.624/ 13.685 | Ixt_lb:  12.685/ 12.737 | vIxt:  18032.373/ 18505.287 | Iyt:  2.257/ 2.261 | \n"
     ]
    }
   ],
   "source": [
    "for runndx in range(n_runs):\n",
    "    for beta in betavals:\n",
    "        if np.isclose(beta,0): \n",
    "            continue\n",
    "        for mode in ['VIB','nlIB',]:\n",
    "            saver.restore(sess, savedir+'/_tf_basemodel')\n",
    "            print(\"Doing %s, beta=%0.4f\" % (mode, beta))\n",
    "            fname = savedir+'/results-%s-%0.5f-run%d' % (mode, beta, runndx)\n",
    "            trainutils.train(sess, mode, beta, cfg, data, n, optimizer, report_every=report_every, fname=fname, fit_var=cfg['initial_fitvar'])\n",
    "\n",
    "            print()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to plot activations\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "if False:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    x, y = data['tst_X'], data['tst_Y']\n",
    "    mx = sess.run(n.encoder[-1], feed_dict={n.x: x})\n",
    "    var = sess.run(n.noisevar)\n",
    "    ax = plt.axes()\n",
    "    for r in mx:\n",
    "        c = plt.Circle((r[0], r[1]), radius=np.sqrt(var), fc='none', alpha=0.05, ec='k')\n",
    "        ax.add_patch(c)\n",
    "    plt.axis('scaled');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
