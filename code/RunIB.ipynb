{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import time, os, pickle, pathlib\n",
    "\n",
    "import model\n",
    "import trainutils\n",
    "import loaddata\n",
    "\n",
    "report_every = 10  # how often to print stats during training\n",
    "n_runs       = 1   # how many times to repeat the whole scan across beta's\n",
    "savedirbase  = str(pathlib.Path().absolute()) + '/saveddata4/'\n",
    "\n",
    "\n",
    "# global_step = tf.Variable(0, trainable=False)\n",
    "# starter_learning_rate = 0.01\n",
    "# learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                           100000, 0.96, staircase=True)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999) \n",
    "\n",
    "runtype = 'MNIST'\n",
    "runtype = 'NoisyClassifier'\n",
    "# runtype = 'Regression'\n",
    "# runtype = 'NoisyClassifierWine'\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'runtype'         : runtype,      # which dataset we're running\n",
    "    'n_batch'         : 128 ,         # SGD batch size\n",
    "    'train_noisevar'  : 'gradient',   # train noise variance with gradient descent ('gradient'), \n",
    "                                      #  scipy optimizer loop ('scipy'), or leave fixed ('none')\n",
    "    'n_noisevar_batch': 1000,         # batch size for training noise variance when train_noisevar='scipy'\n",
    "    'initial_fitvar'  : False,        # whether to set noisevar to optimal value before training\n",
    "    'squaredIB'       : False,        # optimize I(Y;T)-beta*I(X;T) or I(Y;T)-beta*I(X;T)^2 \n",
    "    'err_func'        : 'softmax_ce', # 'softmax_ce' for classification, 'mse' for regression  \n",
    "    'train_kdewidth'  : True,         # whether to adapt the kernel width \n",
    "}\n",
    "#cfg['train_noisevar'] = 'scipy'\n",
    "#cfg['train_kdewidth'] = False\n",
    "\n",
    "\n",
    "betavals = None\n",
    "data     = loaddata.load_data(runtype)\n",
    "\n",
    "if runtype == 'MNIST':\n",
    "    savedir = runtype + '/v1'\n",
    "    cfg.update({\n",
    "        'n_epochs'    : 150,\n",
    "        'squaredIB'   : True,\n",
    "        'encoder_arch': [(512,tf.nn.relu),(512,tf.nn.relu),(2,None)], \n",
    "        'decoder_arch': [(512,tf.nn.relu),(10,None)],\n",
    "    })\n",
    "\n",
    "elif runtype == 'NoisyClassifierWine':\n",
    "    savedir = runtype + '/v1'\n",
    "    cfg.update({\n",
    "        'n_epochs'    : 500,\n",
    "        'encoder_arch': [(10,tf.nn.relu),(10,tf.nn.relu),(10,tf.nn.relu),], \n",
    "        #'encoder_arch': [(10,tf.nn.relu),(10,tf.nn.relu),(3,tf.nn.relu),], \n",
    "        'decoder_arch': [(10,tf.nn.relu),(data['trn_Y'].shape[1],None)],\n",
    "        #'squaredIB'   : True,\n",
    "    })\n",
    "    betavals = 10**np.linspace(-3, 0, 30, endpoint=True)\n",
    "    \n",
    "elif runtype == 'NoisyClassifier':\n",
    "    savedir = runtype + '/v1'\n",
    "    cfg.update({\n",
    "        'n_epochs'      : 300,\n",
    "        'encoder_arch'  : [(20,tf.nn.relu),(20,tf.nn.relu),(2,None)], \n",
    "        'decoder_arch'  : [(20,tf.nn.relu),(2,None)],\n",
    "    })\n",
    "    \n",
    "elif runtype == 'Regression':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0003, beta1=0.9, beta2=0.999) \n",
    "    #savedir = runtype + '/v2' # v5sq-eta'\n",
    "    savedir = runtype + '/v2' # v5sq-eta'\n",
    "    cfg.update({\n",
    "        'n_epochs'         : 1500,\n",
    "        #'encoder_arch'     : [(100,tf.nn.relu),(100,tf.nn.relu),(2,None)], \n",
    "        'encoder_arch'     : [(100,tf.nn.relu),(100,tf.nn.relu),(10,None)], \n",
    "        'decoder_arch'     : [(100,tf.nn.relu),(10,None)],\n",
    "        'err_func'         : 'mse',\n",
    "    })\n",
    "    betavals = 10**np.linspace(-4, 1, 30, endpoint=True)\n",
    "    #betavals = 10**np.linspace(0, 1, 10, endpoint=True)\n",
    "        \n",
    "else:\n",
    "    raise Exception('unknown runtype')\n",
    "    \n",
    "savedir = savedirbase + savedir\n",
    "cfg['optimizer'] = repr(optimizer)\n",
    "\n",
    "if betavals is None:\n",
    "    betavals = 10**np.linspace(-5, 0.1, 30, endpoint=True)\n",
    "#betavals = 10**np.linspace(0.1, 1, 10, endpoint=True)\n",
    "#betavals = np.linspace(0.1, 0.4, 10, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_net():\n",
    "    return model.Net(input_dims   = data['trn_X'].shape[1],\n",
    "                  encoder_arch = cfg['encoder_arch'], \n",
    "                  decoder_arch = cfg['decoder_arch'],\n",
    "                  err_func     = cfg['err_func'],\n",
    "                  entropyY     = data['entropyY'],\n",
    "                  trainable_noisevar = cfg['train_noisevar']=='gradient', \n",
    "                  noisevar     = 0.01,\n",
    "                  kdewidth    = -20.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    print(\"Making base model\")\n",
    "    n = get_net()\n",
    "    saver = tf.train.Saver(max_to_keep=30) # save last 30 epochs\n",
    "    \n",
    "    #cfg2['n_epochs'] = 10\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    trainutils.train(sess, saver, 'ce', 0.0, cfg, data, n, optimizer, report_every, savedir=savedir+'/basemodel')\n",
    "    print(\"Model saved in path: %s\" % savedir)\n",
    "    del n, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for runndx in range(n_runs):\n",
    "    for beta in betavals:\n",
    "        if np.isclose(beta,0): \n",
    "            continue\n",
    "        for mode in ['VIB','nlIB',]:\n",
    "            tf.reset_default_graph()\n",
    "            with tf.Session() as sess:\n",
    "                n = get_net()\n",
    "                saver = tf.train.Saver(max_to_keep=30) # save last 30 epochs\n",
    "                saver.restore(sess, tf.train.latest_checkpoint(savedir+'/basemodel'))\n",
    "                sqmode  = 'sq'  if cfg['squaredIB'] else 'reg'\n",
    "                print(\"Doing %s, beta=%0.4f, %s\" % (mode, beta, sqmode))\n",
    "                trainutils.train(sess, saver, mode, beta, cfg, data, n, optimizer, report_every=report_every, \n",
    "                                 savedir=savedir + '/results-%s-%0.5f-%s-run%d' % (mode, beta, sqmode, runndx), \n",
    "                                 fit_var=cfg['initial_fitvar'])\n",
    "                del saver\n",
    "\n",
    "                print()\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to plot activations\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "if False:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    x, y = data['tst_X'], data['tst_Y']\n",
    "    mx = sess.run(n.encoder[-1], feed_dict={n.x: x})\n",
    "    var = sess.run(n.noisevar)\n",
    "    ax = plt.axes()\n",
    "    for r in mx:\n",
    "        c = plt.Circle((r[0], r[1]), radius=np.sqrt(var), fc='none', alpha=0.05, ec='k')\n",
    "        ax.add_patch(c)\n",
    "    plt.axis('scaled');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
